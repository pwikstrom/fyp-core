{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import generate as ollama_generate\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import join,basename,exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "from os import makedirs\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"AUG29\"\n",
    "key_frame_dir = join(data_dir, \"key_frames\")\n",
    "video_dir = join(data_dir, \"videos\")\n",
    "analysed_video_frames_path = join(data_dir, \"analysed_video_frames.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(data_dir):\n",
    "    print(f\"Creating directory {data_dir}\")\n",
    "    makedirs(data_dir)\n",
    "\n",
    "if not exists(video_dir):\n",
    "    print(f\"Creating directory {video_dir}\")\n",
    "    makedirs(video_dir)\n",
    "\n",
    "if not exists(key_frame_dir):\n",
    "    print(f\"Creating directory {key_frame_dir}\")\n",
    "    makedirs(key_frame_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract key frames from a video file and save them as images in a directory\n",
    "def extract_key_frames(video_path, output_path, threshold=30.0, prefix=None, verbose=False):\n",
    "    from os.path import exists, join\n",
    "    from os import makedirs\n",
    "    from numpy import mean, std, log\n",
    "    import cv2 as cv\n",
    "\n",
    "    if prefix is None:\n",
    "        prefix = \"\"\n",
    "    else:\n",
    "        prefix = f\"{prefix}_\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not exists(output_path):\n",
    "        makedirs(output_path)\n",
    "        \n",
    "    # Capture the video from the file\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "\n",
    "    # get frame rate and total number of frames\n",
    "    fps = cap.get(cv.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count/fps\n",
    "    min_key_frame_separation = max(1, duration/20)\n",
    "\n",
    "    # it is not necessary to check every single frame.\n",
    "    # Some heuristics calculates an appropriate interval\n",
    "    frame_check_interval = int(0.33 * log(10+0.5) * fps)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Frames:{frame_count} | FPS:{fps:.0f} | Dur:{duration:.1f}s | Check interval:{frame_check_interval} | Min key frame separation:{min_key_frame_separation:.1f}s\")\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Cannot read video file\")\n",
    "        return\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    prev_gray = cv.cvtColor(prev_frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize frame count and other things\n",
    "    frame_idx = 0\n",
    "    key_frame_idx_list = []\n",
    "    key_frames = []\n",
    "    key_frame_sharpness_list = []\n",
    "    all_frames_sharpness_list = []\n",
    "    last_saved_frame_idx = 0\n",
    "\n",
    "    # Loop through all frames\n",
    "    while True:\n",
    "        # Read next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Check if it's time to check the frame\n",
    "        if frame_idx % frame_check_interval == 0:\n",
    "\n",
    "            # Convert current frame to grayscale\n",
    "            gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Calculate absolute difference between frames to detect changes\n",
    "            diff = cv.absdiff(prev_gray, gray)\n",
    "            mean_diff = mean(diff)\n",
    "\n",
    "            # Apply Laplacian operator to calculate blurriness/sharpness of the frame\n",
    "            # since we only want to keep reasonably sharp key frames and add to a list of all frames\n",
    "            frame_sharpness_metric = cv.Laplacian(gray, cv.CV_64F).var()\n",
    "            all_frames_sharpness_list.append(frame_sharpness_metric)\n",
    "\n",
    "            \n",
    "\n",
    "            # If diff between the frames is greater than threshold, assume this is a key frame.\n",
    "            # But only if it's been a while since the last key frame or if this is at the beginning\n",
    "            # of the video since some videos start with a black frame or something like that\n",
    "            # Also save the first and last (-ish) frame\n",
    "            if (mean_diff > threshold and ((frame_idx - last_saved_frame_idx > fps * min_key_frame_separation) or (frame_idx < fps*0.5))) or frame_idx == 0 or frame_idx == frame_count - 2:\n",
    "            \n",
    "                # Copy the index into a prev-variable\n",
    "                last_saved_frame_idx = frame_idx\n",
    "\n",
    "                # Add the key frame index to the list of key frame indices\n",
    "                key_frame_idx_list.append(frame_idx)\n",
    "\n",
    "                # Add the key frame to the list of key frames\n",
    "                key_frames.append(frame)\n",
    "\n",
    "                # Add the sharpness metric to the list of sharpness metrics\n",
    "                key_frame_sharpness_list.append(frame_sharpness_metric)\n",
    "\n",
    "                # Update previous gray frame\n",
    "                prev_gray = gray\n",
    "\n",
    "        # Update index counter\n",
    "        frame_idx += 1\n",
    "\n",
    "    # When all frames have been checked, release the video capture\n",
    "    cap.release()\n",
    "\n",
    "    # Calculate mean and standard deviation of sharpness metrics for all frames\n",
    "    mean_sharpness = mean(all_frames_sharpness_list)\n",
    "    std_sharpness = std(all_frames_sharpness_list)\n",
    "\n",
    "    # Calculate the threshold for minimum sharpness based on some heuristics\n",
    "    min_sharpness_threshold = max(mean_sharpness - 1.5 * std_sharpness, mean_sharpness*0.66)\n",
    "\n",
    "    # counter for saved key frames\n",
    "    saved_key_frame_count = 0\n",
    "\n",
    "    # print things\n",
    "    if verbose:\n",
    "        print(f\"Sharpness - Mean:{mean_sharpness:.2f} | Std:{std_sharpness:.2f} | Thresh:{min_sharpness_threshold:.2f}\")\n",
    "\n",
    "    # Loop through key frames\n",
    "    for i, frame_sharpness in enumerate(key_frame_sharpness_list):\n",
    "        # Save key frame if it's sharp enough\n",
    "        if frame_sharpness > min_sharpness_threshold or len(key_frames) <= 5:\n",
    "            key_time = key_frame_idx_list[i]/fps\n",
    "\n",
    "            print(\"S\",end=\"\",flush=True)\n",
    "\n",
    "            # Save frame as JPEG file\n",
    "            filename = join(\n",
    "                output_path, prefix+f\"F{key_frame_idx_list[i]}_T{key_time:.0f}.jpg\")\n",
    "            cv.imwrite(filename, key_frames[i])\n",
    "            saved_key_frame_count += 1\n",
    "        else:\n",
    "            print(\".\",end=\"\",flush=True)\n",
    "\n",
    "\n",
    "    print(f\" | {saved_key_frame_count}/{len(key_frame_idx_list)} key frames saved w prefix {join(output_path, prefix)}\")\n",
    "    return key_frame_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ollama_frame_analysis(a_frame, the_model='llava:latest'):\n",
    "      \n",
    "      #frame_prompt = 'generate a json formatted data structure with the following about this image: main_activity; location; human_count; guessed_genders; observed_ages_of_humans; emotions; ethnicities_observed. If it is not possible to determine the data, respond with unknown'\n",
    "      frame_prompt = \"generate a json with information about the image with this structure: {'list_of_objects_in_image':<str>, 'main_activity':<str>, 'location':<str>, 'human_count':<int>, 'guessed_genders':<str>; 'age_of_humans_observed':<int>; 'emotions':<str>; 'ethnicities_observed':<str>}. If data cannot be determined, return 'unknown'. Only use one of the options happy, sad, angry, surprised, neutral, disgusted, fearful, and calm for emotions.\"\n",
    "      llm_results = ollama_generate(model=the_model, \n",
    "                              prompt=frame_prompt,\n",
    "                              format = 'json',\n",
    "                              images = [a_frame['image']])\n",
    "      a_frame[\"llm_result\"] = json.loads(llm_results[\"response\"])\n",
    "      return a_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frame_details_from_filename(a_path):\n",
    "    from base64 import b64encode\n",
    "\n",
    "    def image_to_base64(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_image = b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        return encoded_image\n",
    "\n",
    "    a_filename = basename(a_path).split(\".\")[0]\n",
    "    parts = a_filename.split(\"_\")\n",
    "    item_id = int(parts[0])\n",
    "    frame_number = int(parts[1][1:])\n",
    "    time = int(parts[2][1:])\n",
    "    return {\"item_id\":item_id, \"frame\":frame_number, \"time\":time, \"image\":image_to_base64(a_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_frame_analysis_results(the_vid_id, combined_frame_analyses):\n",
    "    relevant_data = ['age_of_humans_observed', 'emotions','ethnicities_observed','guessed_genders','human_count','list_of_objects_in_image','location','main_activity']\n",
    "\n",
    "    video_analysis = {\"item_id\":the_vid_id}\n",
    "    for rd in relevant_data:\n",
    "        video_analysis[rd] = np.nan\n",
    "\n",
    "    if combined_frame_analyses[the_vid_id]['frame_count'] == 1:\n",
    "        for u in relevant_data:\n",
    "            if isinstance(combined_frame_analyses[the_vid_id][u],list):\n",
    "                if len(combined_frame_analyses[the_vid_id][u]) > 0:\n",
    "                    video_analysis[u] = combined_frame_analyses[the_vid_id][u][0]\n",
    "                    #print(u,combined_frame_analyses[u][0])\n",
    "                else:\n",
    "                    video_analysis[u] = \"unknown\"\n",
    "            else:\n",
    "                video_analysis[u] = combined_frame_analyses[the_vid_id][u]\n",
    "        return video_analysis\n",
    "\n",
    "\n",
    "    video_analysis[\"human_count\"] = np.mean([u for u in combined_frame_analyses[the_vid_id][\"human_count\"] if isinstance(u,int)==True])\n",
    "\n",
    "    fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"list_of_objects_in_image\"]]\n",
    "    p2 = \"This is a list of objects identified in sorted keyframes of a single video, each keyframe is separated by a semi-colon. Combine the data from the keyframes to describe the objects you see in the video with no more than ten words. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "    nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "    #print(nowthis['response'])\n",
    "    video_analysis['list_of_objects_in_image'] = nowthis['response']\n",
    "\n",
    "    fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"main_activity\"]]\n",
    "    p2 = \"This is a list of activities observed in sorted keyframes of a single video, each keyframe is separated by a semi-colon. Combine the data from the keyframes to describe what is happening in the video with no more than ten words. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "    nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "    #print(nowthis['response'])\n",
    "    video_analysis['main_activity'] = nowthis['response']\n",
    "\n",
    "    fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"location\"]]\n",
    "    p2 = \"This is a list of observed locations in sorted keyframes of a single video, each keyframe is separated by a semi-colon. Combine the data from the keyframes to describe where the video has been recorded with no more than ten words. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "    nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "    #print(nowthis['response'])\n",
    "    video_analysis['location'] = nowthis['response']\n",
    "\n",
    "    if video_analysis[\"human_count\"] > 0:\n",
    "        fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"guessed_genders\"]]\n",
    "        p2 = \"This is a list of the genders of the people observed in sorted keyframes of a single video. Each keyframe is separated by a semi-colon. Combine the data from the keyframes to estimate the most common gender of the people in the video with no more than five words. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "        nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "        #print(nowthis['response'])\n",
    "        video_analysis['guessed_genders'] = nowthis['response']\n",
    "\n",
    "        fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"emotions\"]]\n",
    "        p2 = \"This is a list of the emotions of the people observed in sorted keyframes of a single video. Each keyframe is separated by a semi-colon. Combine the data from the keyframes to describe the most common emotion of the people in the video with no more than five words. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "        nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "        #print(nowthis['response'])\n",
    "        video_analysis['emotions'] = nowthis['response']\n",
    "\n",
    "        fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"ethnicities_observed\"]]\n",
    "        p2 = \"This is a list of the ethnicities of the people observed in sorted keyframes of a single video. Each keyframe is separated by a semi-colon. Combine the data from the keyframes to describe the most common ethnicity of the people in the video with no more than five words. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "        nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "        #print(nowthis['response'])\n",
    "        video_analysis['ethnicities_observed'] = nowthis['response']\n",
    "\n",
    "        fixed_f = [str(d) for d in combined_frame_analyses[the_vid_id][\"age_of_humans_observed\"] if str(d) not in [\"Unknown\",\"unknown\",\"\",\"0\"]]\n",
    "        p2 = \"This is a list of the estimated ages of the people observed in sorted keyframes of a single video. Each keyframe is separated by a semi-colon. Combine the data from the keyframes to qualitatively describe the age of the people in the video with no more than five words. I don't want a list of age estimates, but a single coherent description for the whole video. Do not explain, and do not open up the response with an intro of any kind: \" + \"; \".join(fixed_f)\n",
    "        nowthis = ollama_generate(model='llama3', prompt=p2)\n",
    "        #print(nowthis['response'])\n",
    "        video_analysis['age_of_humans_observed'] = nowthis['response']\n",
    "\n",
    "    return video_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_images_in_row(images):\n",
    "    import cv2 as cv\n",
    "    import base64\n",
    "\n",
    "    # Read images\n",
    "    img_list = []\n",
    "    for img in images:\n",
    "        decoded_image = base64.b64decode(img)#['image'])\n",
    "        np_arr = np.frombuffer(decoded_image, np.uint8)\n",
    "        decoded_image = cv.imdecode(np_arr, cv.IMREAD_COLOR)\n",
    "        decoded_image = cv.cvtColor(decoded_image, cv.COLOR_BGR2RGB)\n",
    "        img_list += [decoded_image]\n",
    "    \n",
    "    # Resize images to have the same height\n",
    "    height = min(img.shape[0] for img in img_list)\n",
    "    resized_imgs = [cv.resize(img, (int(img.shape[1] * height / img.shape[0]), height)) for img in img_list]\n",
    "\n",
    "    # Concatenate images horizontally\n",
    "    concatenated_img = np.hstack(resized_imgs)\n",
    "\n",
    "    # Display the concatenated image using matplotlib\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(concatenated_img)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here on we are working with the videos found in the videos directory.\n",
    "videos_saved = [int(vid.split(\".\")[0]) for vid in listdir(video_dir) if vid.endswith(\".mp4\")]\n",
    "print(f\"{len(videos_saved):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_video_files = [join(video_dir,fn) for fn in listdir(video_dir) if fn.endswith(\".mp4\")]\n",
    "for i,an_item in enumerate(videos_saved):\n",
    "    print(f\"{i} ({an_item})\", end=\": \", flush=True)\n",
    "\n",
    "    extracted_key_frames = [fn for fn in listdir(key_frame_dir) if fn.endswith(\".jpg\")]\n",
    "    if any([f\"{an_item}\" in fn for fn in extracted_key_frames]):\n",
    "        print(f\"key frames already extracted\")\n",
    "    else:\n",
    "        extract_key_frames(join(video_dir,f\"{an_item}.mp4\"), key_frame_dir, prefix=f\"{an_item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a list of dictionaries to hold the details for each frame connected with the videos saved\n",
    "all_frames = []\n",
    "for i,an_item in enumerate(videos_saved):\n",
    "    print(\".\",end=\"\",flush=True)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print()\n",
    "\n",
    "    all_frames += [extract_frame_details_from_filename(\n",
    "        join(key_frame_dir, fn)) for fn in listdir(key_frame_dir) if str(an_item) in fn]\n",
    "print(f\"\\nWe have {len(all_frames)} frames from the {len(videos_saved)} videos\")\n",
    "\n",
    "all_frames_df = pd.DataFrame(all_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load already analysed frames from file\n",
    "analysed_frames = []\n",
    "\n",
    "if exists(analysed_video_frames_path):\n",
    "    with open(analysed_video_frames_path, 'r') as file:\n",
    "        analysed_frames = json.load(file)\n",
    "    print(f\"{len(analysed_frames)} analysed video frames loaded from file\")\n",
    "else:\n",
    "    print(f\"No video frames has been analysed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when piloting this code, select a few videos and frames to work with\n",
    "selected_videos = list(map(lambda x: int(x), np.random.choice(videos_saved, 3,replace=False)))\n",
    "print(selected_videos)\n",
    "\n",
    "selected_frames = [f for f in all_frames if f[\"item_id\"] in selected_videos]\n",
    "print(f\"Analysing {len(selected_frames)} frames selected from {len(selected_videos)} videos\")\n",
    "\n",
    "\n",
    "# add the frame analysis to the selected frames - this can be pretty slow (about 6-12s per frame)\n",
    "frame_ids = [f\"{af['item_id']}_{af['frame']}\" for af in analysed_frames]\n",
    "\n",
    "for i,sf in enumerate(selected_frames):\n",
    "    if not f\"{sf['item_id']}_{sf['frame']}\" in frame_ids:\n",
    "        if True:#try:\n",
    "            analysed_frames += [generate_ollama_frame_analysis(sf)]\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        #except:\n",
    "        #    print(\"#\", end=\"\", flush=True)\n",
    "    else:\n",
    "        print(\"^\", end=\"\", flush=True)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print()\n",
    "\n",
    "for af in analysed_frames:\n",
    "    if \"image\" in af.keys():\n",
    "        del af[\"image\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysed_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary as a JSON file\n",
    "with open(analysed_video_frames_path, \"w\") as json_file:\n",
    "    json.dump(analysed_frames, json_file)\n",
    "\n",
    "print(f\"{len(analysed_frames):,} analysed video frames file saved successfully as {analysed_video_frames_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the analysed frames into dicts by item_id\n",
    "analysed_frames_organised_as_videos = {}\n",
    "\n",
    "for d in analysed_frames:\n",
    "    if not d['item_id'] in analysed_frames_organised_as_videos:\n",
    "        analysed_frames_organised_as_videos[d['item_id']] = []\n",
    "    analysed_frames_organised_as_videos[d['item_id']] += [d['llm_result']]\n",
    "    analysed_frames_organised_as_videos[d['item_id']][-1].update({\"frame_ts\": d[\"time\"]})\n",
    "for v in analysed_frames_organised_as_videos:\n",
    "    analysed_frames_organised_as_videos[v] = sorted(analysed_frames_organised_as_videos[v], key=lambda x: x[\"frame_ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the analysed frames into dicts by item_id\n",
    "analysed_frames_organised_as_videos = {}\n",
    "\n",
    "for d in analysed_frames:\n",
    "    if not d['item_id'] in analysed_frames_organised_as_videos:\n",
    "        analysed_frames_organised_as_videos[d['item_id']] = {\"frames\":[]}\n",
    "    analysed_frames_organised_as_videos[d['item_id']][\"frames\"] += [d['llm_result']]\n",
    "    analysed_frames_organised_as_videos[d['item_id']][\"frames\"][-1].update({\"frame_ts\": d[\"time\"]})\n",
    "for v in analysed_frames_organised_as_videos:\n",
    "    analysed_frames_organised_as_videos[v][\"frames\"] = sorted(analysed_frames_organised_as_videos[v][\"frames\"], key=lambda x: x[\"frame_ts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the results for each video into a single dict\n",
    "analysed_frames_combined = {}\n",
    "for item_id in analysed_frames_organised_as_videos:\n",
    "    combined_results = {\"frame_count\":len(analysed_frames_organised_as_videos[item_id]['frames'])}\n",
    "    for b in analysed_frames_organised_as_videos[item_id]['frames']:\n",
    "        for k in b:\n",
    "            if k not in combined_results:\n",
    "                combined_results[k] = []\n",
    "            if b[k] not in [\"Unknown\",\"unknown\",\"\"]:\n",
    "                hopp = b[k]\n",
    "                if isinstance(hopp, list):\n",
    "                    hopp = hopp[0]\n",
    "                if isinstance(hopp, dict):\n",
    "                    hopp = list(hopp.values())[0]\n",
    "                try:\n",
    "                    hopp = int(hopp)\n",
    "                except:\n",
    "                    pass\n",
    "                if isinstance(hopp, str):\n",
    "                    hopp = hopp.lower().replace(\"-\",\" \")\n",
    "                combined_results[k].append(hopp)\n",
    "    \n",
    "    analysed_frames_combined[item_id] = combined_results\n",
    "\n",
    "# combine the results for each video into a single dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analysis = []\n",
    "for item in analysed_frames_combined:\n",
    "    print(f\"{item} - {analysed_frames_combined[item]['frame_count']} frames\")\n",
    "    video_analysis += [combine_frame_analysis_results(item, analysed_frames_combined)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_analysis_df = pd.DataFrame(video_analysis)\n",
    "print(video_analysis_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_vid_id = video_analysis_df.item_id.sample().values[0]\n",
    "print(tabulate(video_analysis_df[video_analysis_df.item_id==the_vid_id].T, headers=\"keys\", tablefmt=\"pipe\"))\n",
    "list_of_images = list(all_frames_df[all_frames_df[\"item_id\"]==the_vid_id].sort_values(\"frame\")[\"image\"].values)\n",
    "if True or all([type(h)==\"str\" for h in list_of_images]):\n",
    "    display_images_in_row(list_of_images)\n",
    "else:\n",
    "    print(\"**** ERROR finding key frames\")\n",
    "Video(join(video_dir,f\"{the_vid_id}.mp4\"), height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
